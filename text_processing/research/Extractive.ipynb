{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    raw_sentences = text.split('. ')\n",
    "    cleaned_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        cleaned_sentences.append(raw_sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sentence_1, sentence_2):\n",
    "    sentence_1_lower = [w.lower() for w in sentence_1]\n",
    "    sentence_2_lower = [w.lower() for w in sentence_2]\n",
    "    \n",
    "    all_words = list(set(sentence_1_lower + sentence_2_lower))\n",
    "    \n",
    "    vector_1 = [0] * len(all_words)\n",
    "    vector_2 = [0] * len(all_words)\n",
    "    \n",
    "    for word in sentence_1_lower:\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        vector_1[all_words.index(word)] += 1\n",
    "    \n",
    "    for word in sentence_2_lower:\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        vector_2[all_words.index(word)] += 1\n",
    "    \n",
    "    return 1 - cosine_distance(vector_1, vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):\n",
    "    summarize_text = []\n",
    "    sentences = get_sentences(text)\n",
    "    \n",
    "    top_n = int(len(sentences) * 0.5)\n",
    "    \n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    \n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    ranked_sentence = sorted(((scores[i], s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    return \". \".join(summarize_text) + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "--------------------------------------------------\n",
      "The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm\\u2019s ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, ands text classification tasks.\"\"\"\n",
    "\n",
    "print('Summary')\n",
    "print('-'*50)\n",
    "print(generate_summary(sample_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
