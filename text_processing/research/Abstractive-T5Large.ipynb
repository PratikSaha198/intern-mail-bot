{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):\n",
    "    preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "    t5_prepared_Text = \"summarize: \" + preprocess_text\n",
    "    tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=8,\n",
    "                                    no_repeat_ngram_size=10,\n",
    "                                    min_length=75,\n",
    "                                    max_length=100,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    output_sentences = output.split('. ')    \n",
    "    result = []\n",
    "    for sentence in output_sentences:\n",
    "        sentence = sentence.capitalize()\n",
    "        result.append(sentence)        \n",
    "    \n",
    "    return \". \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "--------------------------------------------------\n",
      "Semi-supervised learning is based on a gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph. The learning problem is then formulated in terms of a gaussian random field. The mean of the field is characterized in terms of harmonic functions.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm\\u2019s ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, ands text classification tasks.\"\"\"\n",
    "\n",
    "summary = generate_summary(sample_text)\n",
    "\n",
    "print('Summary')\n",
    "print('-'*50)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "--------------------------------------------------\n",
      "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. The model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: flickr9k, flickr30k and ms coco.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.\"\n",
    "\n",
    "summary = generate_summary(sample_text)\n",
    "\n",
    "print('Summary')\n",
    "print('-'*50)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
